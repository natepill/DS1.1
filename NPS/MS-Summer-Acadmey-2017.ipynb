{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS Summer Academy NPS Data Exploration (2016-2017)\n",
    "\n",
    "## Questsions Asked:\n",
    "\n",
    "    * How many promoters, passives, and detractors are there in both years? How do the scores differ by year?\n",
    "    * What track of students had the best experience at the summer academy? What about the worst experience?\n",
    "    * Did students feel as though the pacing increased as the program went on?\n",
    "    * Which location had the best overall experience?\n",
    "    * Did students at the NY location have a better or worse expeience as the program went on?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General NPS Cleaning Process:\n",
    "\n",
    "- Step 1: Import, Clean, and Aggregate Weeks 1-7 for 2016 Data\n",
    "- Step 2: Import and Clean Week 8 for 2016 Data\n",
    "- Step 2.5: Aggregate Weeks 1-7 with Week 8 to produce Full 2016 Dataset\n",
    "- Step 3: Import and Clean 2017 Data\n",
    "- Step 3.5: Aggregate Full 2016 Data with 2017 Data to produce Complete Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promoters, Passives, Detractors for 2017: 78, 34, 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas is a library for basic data analysis\n",
    "import pandas as pd\n",
    "\n",
    "# NumPy is a library for advanced mathematical computation\n",
    "import numpy as np\n",
    "\n",
    "# MatPlotLib is a library for basic data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SeaBorn is a library for advanced data visualization\n",
    "import seaborn as sns\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Stretch Challenge_:\n",
    "\n",
    "### Functionalize Data Manipulation code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\", context=\"notebook\", palette=\"deep\")\n",
    "\n",
    "COLOR_COLUMNS = [\"#66C2FF\", \"#5CD6D6\", \"#00CC99\", \"#85E085\", \"#FFD966\", \"#FFB366\", \"#FFB3B3\", \"#DAB3FF\", \"#C2C2D6\"]\n",
    "\n",
    "sns.set_palette(palette=COLOR_COLUMNS, n_colors=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data Cleaning and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REL_PATH_DIRECTORY = \"../datasets/SA_Feedback_Surveys_FINAL/2016/\"\n",
    "ALL_BUT_8_PATH = \"Anon*.csv\"\n",
    "\n",
    "THE_8_PATH = \"Week 8 Feedback (2016, incomplete) - results.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weeks 1-7 (2016)\n",
    "\n",
    "- NOTE: Data is _slightly_ different across various weeks and locations. **Approach with caution!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_but_8_2016_files = glob.glob(REL_PATH_DIRECTORY + ALL_BUT_8_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 7 Feedback - Taipei.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 6 Feedback - Tokyo.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 1 Feedback - Singapore.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 7 Feedback - LA.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 4 Feedback - SF.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 5 Feedback - SV.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 4 Feedback - SG.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 6 Feedback - NY.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 5 Feedback - HK.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 1 Feedback - SF.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 2 Feedback - LA.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 6 Feedback - Taipei.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 3 Feedback - NY.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 6 Feedback - LA.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 5 Feedback - SF.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 5 Feedback - SG.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 4 Feedback - SV.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 7 Feedback - NY.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 3 Feedback - LA.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 2 Feedback - NY.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 1 Feedback - SV.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 5 Feedback - LA.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 6 Feedback - SF.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 6 Feedback - SG.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 7 Feedback - SV.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 4 Feedback - NY.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 3 Feedback - SF.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 1 Feedback - NY.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 2 Feedback - SV.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 3 Feedback - SG.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 4 Feedback - LA.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 7 Feedback - SF.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 6 Feedback - SV.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 6 Feedback - HK.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 5 Feedback - NY.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 2 Feedback - SF.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 1 Feedback - LA.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 2 Feedback - SG.csv',\n",
       " '../datasets/SA_Feedback_Surveys_FINAL/2016/Anon Week 3 Feedback - SV.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_but_8_2016_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOAL: Place _Week_ and _Location_ data in each 2016 (not Week 8) DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Anon Week 1 Feedback - LA.csv\"\n",
    "fileparts = filename.split(\" \")\n",
    "\n",
    "test_file_path = REL_PATH_DIRECTORY + filename\n",
    "df_test = pd.read_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_num, location = int(fileparts[2]), fileparts[5].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_files():\n",
    "    data_arr = list()\n",
    "    for filename in all_but_8_2016_files:\n",
    "        # When reading each file, we grab the Week & Location data and add it to two new columns\n",
    "        week, location = _get_week_and_location(filename)\n",
    "        df = pd.read_csv(filename)\n",
    "        df[\"Week\"], df[\"Location\"] = week, location\n",
    "        data_arr.append(df)\n",
    "    return data_arr\n",
    "\n",
    "def _get_week_and_location(filename):\n",
    "    fileparts = filename.split(\" \")\n",
    "    week, location = int(fileparts[2]), fileparts[5].split(\".\")[0]\n",
    "    return week, location\n",
    "\n",
    "data = read_all_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: The bad dataframe with `Unnamed: 0` is the `Anon Week 1 Feedback - SV.csv` DF\n",
    "\n",
    "The `Unnamed: 0` column/feature is measuring Timestamp data (Datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: The bad dataframe with no `Timestamp` or `Unnamed: 0` is the `Anon Week 5 Feedback - SF.csv` DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Finds the dataframe with no Timestamp or Unnamed: 0 column and adds a Timestamp column with default string values'''\n",
    "\n",
    "youre_the_one = None\n",
    "index_of_df = None\n",
    "for index, df in enumerate(data):\n",
    "    if \"Unnamed: 0\" not in df.columns and \"Timestamp\" not in df.columns:\n",
    "        df.insert(0, 'Timestamp', 'NO TIMESTAMP')\n",
    "        youre_the_one = df\n",
    "        index_of_df = index\n",
    "        \n",
    "\n",
    "# youre_the_one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>What track are you in?</th>\n",
       "      <th>How would you rate your overall satisfaction with the Summer Academy this week?</th>\n",
       "      <th>How well are the tutorials paced?</th>\n",
       "      <th>Week</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6/24/2016 13:32:01</td>\n",
       "      <td>Games</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6/24/2016 13:48:09</td>\n",
       "      <td>Apps</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6/24/2016 13:49:23</td>\n",
       "      <td>Apps</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>SV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Timestamp What track are you in?  \\\n",
       "0  6/24/2016 13:32:01                  Games   \n",
       "1  6/24/2016 13:48:09                   Apps   \n",
       "2  6/24/2016 13:49:23                   Apps   \n",
       "\n",
       "   How would you rate your overall satisfaction with the Summer Academy this week?  \\\n",
       "0                                                  2                                 \n",
       "1                                                  3                                 \n",
       "2                                                  3                                 \n",
       "\n",
       "   How well are the tutorials paced?  Week Location  \n",
       "0                                  4     1       SV  \n",
       "1                                  4     1       SV  \n",
       "2                                  3     1       SV  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Renamed Unnamed: 0 ---> Timestamp'''\n",
    "\n",
    "data[20] = data[20].rename(columns={'Unnamed: 0': 'Timestamp'})\n",
    "\n",
    "data[20].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Timestamp': 39,\n",
       " 'How would you rate your overall satisfaction with the Summer Academy this week?': 39,\n",
       " 'How well is the schedule paced?': 33,\n",
       " 'Week': 39,\n",
       " 'Location': 39,\n",
       " 'How well are the tutorials paced?': 6,\n",
       " 'What track are you in?': 24}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# week_one_through_seven_df = pd.DataFrame(index=index, columns=columns)\n",
    "# Find a way to grab `week` and `location` data from filenames\n",
    "# Find all potential unique columns\n",
    "# Use unique columns to create master DF and put all data into that one (merging, copies)\n",
    "\n",
    "\n",
    "#Histogram of columns and their frequency\n",
    "column_names = dict()\n",
    "\n",
    "for df in data:\n",
    "    for column in df.columns:\n",
    "        if column in column_names:\n",
    "            column_names[column] += 1\n",
    "        else:\n",
    "            column_names[column] = 1\n",
    "            \n",
    "column_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Aggregate Schedule and Tutorial data into _Pacing_ column (mutual exclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Each 2016 (not Week 8) DF for Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functionalized grabbing unique values\n",
    "\n",
    "def check_unique_values_by_column(col_name):\n",
    "    unique_values = set()\n",
    "    for df in data:\n",
    "        if col_name in df.columns:\n",
    "            df_vals = df[col_name].unique().tolist()\n",
    "            unique_values.update(df_vals)\n",
    "    return unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Timestamp',\n",
       " 'How would you rate your overall satisfaction with the Summer Academy this week?',\n",
       " 'How well is the schedule paced?',\n",
       " 'Week',\n",
       " 'Location',\n",
       " 'How well are the tutorials paced?',\n",
       " 'What track are you in?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_name_list = list(column_names.keys())\n",
    "col_name_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionary of Unique Values per Column Name over All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques_dict = dict()\n",
    "# For each column name in our list above, we create a dictionary element where...\n",
    "# KEY: Column Name\n",
    "# VALUE: All Unique Values for that Column Name across All Data\n",
    "for index in range(7):\n",
    "    col_name = col_name_list[index]\n",
    "    uniques_dict[col_name] = check_unique_values_by_column(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = list(uniques_dict[\"How would you rate your overall satisfaction with the Summer Academy this week?\"])\n",
    "uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''For satisfaction column, map errors  to 0, then change datatype of column to type int '''\n",
    "  \n",
    "satisfaction_map = dict(zip(uniques, [1, 2, 3, 4, 5, 4, 3, 5, 0]))\n",
    "\n",
    "satisfaction_column = 'How would you rate your overall satisfaction with the Summer Academy this week?'\n",
    "\n",
    "for df in data:\n",
    "    if 'How would you rate your overall satisfaction with the Summer Academy this week?' in df:\n",
    "#         if '#REF!' in df[satisfaction_column]:\n",
    "        df['How would you rate your overall satisfaction with the Summer Academy this week?'] = df['How would you rate your overall satisfaction with the Summer Academy this week?'].map(satisfaction_map).astype(int)\n",
    "#         df['How would you rate your overall satisfaction with the Summer Academy this week?'] = df['How would you rate your overall satisfaction with the Summer Academy this week?'].astype(int)\n",
    "            \n",
    "\n",
    "\n",
    "for df in data:\n",
    "    if 'How would you rate your overall satisfaction with the Summer Academy this week?' in df:\n",
    "        print(df['How would you rate your overall satisfaction with the Summer Academy this week?'].unique(), \"\\n\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016 NOT Week 8 data dictionary mappings \n",
    "\n",
    "''' Mapping Schedule pacing to convert np.NaN and errors to 0 so that we can only deal with values greater than 0 in \n",
    "our calculations and making sure values are integers. '''\n",
    "\n",
    "'''Our collection of mapping dictionaries to deal with np.NaN, #REF!, and wrong datatypes'''\n",
    "\n",
    "schedule_pacing_map = {\n",
    "    '#REF!': 0,\n",
    "    np.NaN: 0, \n",
    "    1: 1, \n",
    "    2: 2, \n",
    "    '2': 2, \n",
    "    3: 3, \n",
    "    '3': 3, \n",
    "    '4': 4, \n",
    "    4: 4, \n",
    "    5: 5, \n",
    "    '5': 5\n",
    "}\n",
    "\n",
    "\n",
    "for df in data:\n",
    "    if \"How well is the schedule paced?\" in df:\n",
    "        df[\"How well is the schedule paced?\"] = df[\"How well is the schedule paced?\"].map(schedule_pacing_map)\n",
    "        df = df.drop(df[df[\"How well is the schedule paced?\"] > 0].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in data:\n",
    "    if 'How would you rate your overall satisfaction with the Summer Academy this week?' in df:\n",
    "        print(df['How would you rate your overall satisfaction with the Summer Academy this week?'].unique(), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Timestamp',\n",
       " 'How would you rate your overall satisfaction with the Summer Academy this week?',\n",
       " 'How well is the schedule paced?',\n",
       " 'Week',\n",
       " 'Location',\n",
       " 'How well are the tutorials paced?',\n",
       " 'What track are you in?']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_types = set()\n",
    "\n",
    "for item in check_unique_values_by_column(col_name_list[0]):\n",
    "    unique_types.update([type(item)])\n",
    "    \n",
    "# unique_types\n",
    "print(col_name_list[0], \": \", list(unique_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks all unique datatypes across every feature across every dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_values_by_column(col_name):\n",
    "    unique_values = set()\n",
    "    for df in data:\n",
    "        if col_name in df.columns:\n",
    "            df_vals = df[col_name].unique().tolist()\n",
    "            unique_values.update(df_vals)\n",
    "    return unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_of_unique_types():\n",
    "    \n",
    "    '''Returns a dict (KEY: Unique Columns in all dataframes, VALUE: Set of unique value types)'''\n",
    "    type_dict = dict()\n",
    "\n",
    "    for feature in range(len(col_name_list)):\n",
    "        unique_types = set()\n",
    "\n",
    "        for item in check_unique_values_by_column(col_name_list[feature]):\n",
    "            unique_types.update([type(item)])\n",
    "\n",
    "        type_dict[col_name_list[feature]] = list(unique_types)\n",
    "        \n",
    "type_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'What track are you in?' looks pretty clean\n",
    "unique_track_vals = check_unique_values_by_column('What track are you in?')\n",
    "unique_track_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Mapper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_types_to_value(df, column_name, type_to_change, change_to_value):\n",
    "    '''Takes a Pandas Dataframe, column name, object type to change, and the value to replace with\n",
    "        Returns a \"mapped\" dataframe based on value types that you want to replace'''\n",
    "    for index, item in df.iterrows():\n",
    "        changer = None\n",
    "        if type(item[0]) == type_to_change:\n",
    "            changer = change_to_value\n",
    "            return df.set_value(index, column_name, changer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Example of using the Type Mapper Function\"\n",
    "\n",
    "johnny = pd.DataFrame(data=[\"asdf\", \"zxcv\", \"qwer\", \"pou\", \"ncmv\", True], columns=[\"chen\"])\n",
    "\n",
    "mapped = map_types_to_value(johnny, \"chen\", bool, \"WORKING\")\n",
    "mapped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Takes NaN values in Timestamp column and maps then to a \"NO TIMESTAMP\" string'''\n",
    "\n",
    "'''CURRENTLY NOT CATCHING NAN VALUE, DONT KNOW WHY!'''\n",
    "\n",
    "for df in data:\n",
    "    map_types_to_value(df, 'Timestamp', np.nan, \"NO TIMESTAMP\")\n",
    "\n",
    "unique_timestamp_vals = check_unique_values_by_column('Timestamp')\n",
    "# unique_timestamp_vals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Merging Mutually Exclusive Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([1, 0, 3, 4, np.nan, np.nan, 0, 6, np.nan, np.nan, np.nan], columns=[\"a\"])\n",
    "df2 = pd.DataFrame([np.nan, np.nan, np.nan, np.nan, 5, 7, np.nan, np.nan, 2, 9, 8], columns=[\"b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Replace all **NaNs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"a\"] = df1[\"a\"].fillna(0)\n",
    "df2[\"b\"] = df2[\"b\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df1['a'] + df2['b']\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1, s2 = \"New York\", \"No Location\"\n",
    "s1 + s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacing_name1 = 'How well is the schedule paced?'\n",
    "pacing_name2 = 'How well are the tutorials pacecd?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we know that the `Schedule Pacing` and `Tutorial Pacing` are mutually exclusive columns, we can simply _ADD_ them together into a new column.\n",
    "\n",
    "### Let's do this last when we have everything in the same dataframe because all we need to do is:\n",
    "    *Map alls NaNs to 0 for both pacing columns\n",
    "    *Just add the dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''FAILED ATTEMPTS AT TRYING TO MERGE PACING COLUMNS INTO ONE COLUMN BY UNNECECARILY DIFFICULT MEANS'''\n",
    "'''BUT TOO MUCH CODE WAS WRITTEN TO DELETE THIS SO IM KEEPING IT HERE, WHY AM I SHOUTING?'''\n",
    "\n",
    "# pacing_df =  pd.DataFrame(columns= ['How well is the schedule paced?','How well are the tutorials paced?'])\n",
    "\n",
    "# array_of_columns = list()\n",
    "\n",
    "\n",
    "# for df in data:\n",
    "#     if 'How well is the schedule paced?' in df.columns:\n",
    "#         array_of_columns.append(df['How well is the schedule paced?'])\n",
    "#         nan_df = pd.DataFrame(np.nan, index=df['How well is the schedule paced?'].size, columns='How well are the tutorials paced?')\n",
    "#         array_of_columns.append(nan_df)\n",
    "# #         pacing_df['How well is the schedule paced?'] = pacing_df['How well is the schedule paced?'] + df['How well is the schedule paced?']\n",
    "#     elif 'How well are the tutorials paced?' in df.columns:\n",
    "#         array_of_columns.append(df['How well are the tutorials paced?'])\n",
    "#         nan_df = pd.DataFrame(np.nan, index=df['How well are the tutorials paced?'].size, columns='How well are the tutorials paced?')\n",
    "#         array_of_columns.append(nan_df)\n",
    "        \n",
    "        \n",
    "# #         pacing_df[ 'How well are the tutorials paced?'] = pacing_df['How well are the tutorials paced?'] + df['How well are the tutorials paced?']\n",
    "        \n",
    "# pacing_df\n",
    "\n",
    "\n",
    "\n",
    "# # ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# pacing_df =  pd.DataFrame(columns= ['How well is the schedule paced?','How well are the tutorials paced?'])\n",
    "# p1 = list()\n",
    "# p2 = list()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for df in data:\n",
    "#     if 'How well is the schedule paced?' in df.columns:\n",
    "#         p1.append(df['How well is the schedule paced?'].values)\n",
    "#         temp_arr = [p1.append(i) for i in df['How well is the schedule paced?'].values]\n",
    "#         p2.append(np.nan * (df['How well is the schedule paced?'].size))\n",
    "# #         p1.append(df['How well is the schedule paced?'].values)\n",
    "\n",
    "#     elif 'How well are the tutorials paced?' in df.columns:\n",
    "#         temp_arr = [p2.append(i) for i in df['How well are the tutorials paced?'].values]\n",
    "#         p2.append(np.nan * (df['How well are the tutorials paced?'].size))\n",
    "\n",
    "        \n",
    "# #         pacing_df[ 'How well are the tutorials paced?'] = pacing_df['How well are the tutorials paced?'] + df['How well are the tutorials paced?']\n",
    "        \n",
    "# p1\n",
    "# # p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 8 (2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_week_8 = pd.read_csv(REL_PATH_DIRECTORY + THE_8_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_week_8[\"Week\"] = 8\n",
    "df_week_8.rename(columns={\"location\": \"Location\"}, inplace=True)\n",
    "df_week_8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = \"../datasets/SA_Feedback_Surveys_FINAL/2017/Student_Feedback_Surveys_Superview.csv\"\n",
    "df_2017 = pd.read_csv(FILEPATH)\n",
    "\n",
    "# 9 or 10 are promoters\n",
    "# 7-8 are passives\n",
    "# 0-6 are detractors\n",
    "\n",
    "# df_2017.head()\n",
    "\n",
    "# Clean Week data to remove redundant \"week\"\n",
    "\n",
    "week_mapper = {\n",
    "    \"Week 1\": 1,\n",
    "    \"Week 2\": 2,\n",
    "    \"Week 3\": 3,\n",
    "    \"Week 4\": 4,\n",
    "    \"Week 5\": 5,\n",
    "    \"Week 6\": 6,\n",
    "    \"Week 7\": 7,\n",
    "    \"Week 8\": 8\n",
    "}\n",
    "\n",
    "df_2017[\"Week\"] = df_2017[\"Week\"].map(week_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Pacing Strings to Numerical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017[\"Schedule Pacing\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacing_mapper = {\n",
    "    'Way too slow': 1,\n",
    "    'A little too slow': 2,\n",
    "    'Just right': 3,\n",
    "    'A little too fast': 4,\n",
    "    'Way too fast': 5,\n",
    "    np.nan: 0\n",
    "}\n",
    "\n",
    "df_2017[\"Schedule Pacing\"] = df_2017[\"Schedule Pacing\"].map(pacing_mapper)\n",
    "df_2017[\"Schedule Pacing\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = pd.read_csv(FILEPATH)\n",
    "df_2017 = df_2017[df_2017[\"Rating (Num)\"] != \"#ERROR!\"]\n",
    "df_2017[\"Rating (Num)\"] = df_2017[\"Rating (Num)\"].astype(int)\n",
    "\n",
    "df_promoters = df_2017.loc[(df_2017['Rating (Num)'] >= 9) & (df_2017['Week'] == 'Week 7')]\n",
    "df_passives = df_2017.loc[(df_2017['Rating (Num)'] >= 7) & (df_2017['Rating (Num)'] <= 8) & (df_2017['Week'] == 'Week 7')]\n",
    "df_detractors = df_2017.loc[(df_2017['Rating (Num)'] < 7) & (df_2017['Week'] == 'Week 7')]\n",
    "\n",
    "\n",
    "# df_promoters\n",
    "# df_passives\n",
    "# df_detractors   \n",
    "\n",
    "# len(df_promoters) #78\n",
    "# len(df_passives) #34\n",
    "# len(df_detractors) #8\n",
    "\n",
    "# num_of_promoters = len(df_promoters)\n",
    "# num_of_promoters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two choices for Pro-Pas-Det Data Divisions:\n",
    "\n",
    "- **Divide** up your promoters, passives, and detractors into _three_ independent DataFrames\n",
    "- **Convert** your logic for promoter, passive, and detractor identifiation into _arguments_ that you can pass to your global DataFrame at anytime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promoter_count = 0\n",
    "passive_count = 0\n",
    "detractor_count = 0\n",
    "\n",
    "index = []\n",
    "columns = []\n",
    "\n",
    "promoter_df = pd.DataFrame(index=index, columns=columns)\n",
    "passive_df = pd.DataFrame(index=index, columns=columns)\n",
    "detractor_df = pd.DataFrame(index=index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_promoter = (df[\"Rating (Num)\"] >= 9)\n",
    "promoters = df[arg_promoter]\n",
    "\n",
    "\n",
    "# week_one = (promoters[\"Week\"] == \"Week 1\")\n",
    "\n",
    "# week_one\n",
    "\n",
    "\n",
    "# len(promoters)\n",
    "# promoters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2017.loc[:, ['ID', 'Track', 'Week', 'Rating (Num)']]\n",
    "\n",
    "\n",
    "all_students = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
